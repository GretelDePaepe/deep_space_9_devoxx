{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32ffacf-9f8b-41e2-9fcd-b0437540f168",
   "metadata": {},
   "source": [
    "# Train a Transformer Model from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7cf657-1ce1-4ee8-83a9-f35076dae939",
   "metadata": {},
   "source": [
    "source: https://huggingface.co/blog/how-to-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ef452-99e3-4d05-8360-5466270e05c0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ef31c3-bb0b-410e-a562-f29f0528fa13",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8fde1-c62d-4e54-9c3f-8987c3bff957",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Deep Space 9 Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6d499d-9478-4283-b42f-0e57eb47bf3a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "PATH_DATA = 'ds9_data/'\n",
    "paths = [str(x) for x in Path(PATH_DATA).glob(\"**/*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7ad9e0-cc8e-411c-83ca-5477d8f00211",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "with open(PATH_DATA + \"all_scripts.txt\", \"w\") as outfile:\n",
    "    for filename in paths:\n",
    "        with open(filename) as infile:\n",
    "            contents = infile.read()\n",
    "            contents = re.sub(\"\\t\",' ',contents)\n",
    "            contents = re.sub(' +',' ',contents)\n",
    "            contents = re.sub('\\n \\n','\\n',contents)\n",
    "            contents = re.sub('\\n+','\\n',contents)\n",
    "            outfile.write(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48600677-e72f-48f1-9000-15d4cc8082e3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374224c4-a568-4946-b14f-0a3015a471f9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "144f96d2-d98d-443c-b81f-44e5e8d6b417",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ds9_scratch/vocab.json', 'ds9_scratch/merges.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "PATH_MODEL = 'ds9_scratch'\n",
    "tokenizer.save_model(PATH_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc35d7a-870e-417b-9ce3-47063e4c1ef2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "495f01b7-8ef2-4eac-9088-b38e1fc068ba",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep 28 06:26:10 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   75C    P0    32W /  70W |      0MiB / 15360MiB |     10%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "453039a2-1527-40aa-80d5-37d4b7c0a255",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f11140-8835-4247-91c7-4e180df9222a",
   "metadata": {},
   "source": [
    "## Train Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5833eb04",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20971250-887a-4c2a-85e1-d0472c5298a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a94994-3907-49e3-946e-66a0116b4fc1",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(PATH_MODEL, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f07b629a-555d-45cf-af2f-65449e8c65f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa4db638-4d9b-41d5-a052-9563464565be",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83504416"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25e52ca0-0d98-44c8-9c28-8c6a90569930",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 s, sys: 1.21 s, total: 23.5 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=PATH_DATA + 'all_scripts.txt',\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "086fe37f-7ac5-4c09-96b3-9ec6490164fd",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([   0,  912,   16,  315, 7944,  284, 3686,  271, 3907,  225,    2])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Rom, you forgot to include the profit </s>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[301])\n",
    "tokenizer.decode(dataset[301][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65fbed46-c4c4-4401-9e35-910463e8e28b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ab26d-cd4f-4a65-a8fc-d8aa7050fc53",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Train for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a75e8ffd-7af6-4aff-85b0-469ffe9ed5fb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(PATH_MODEL, \"transformer\"),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=1000,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b088f2df-3b63-4049-8598-b01fb8e6030d",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 388834\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6076\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6076' max='6076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6076/6076 25:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.702300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.615800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.180200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.888300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.556700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.435100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.300300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.161200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.143100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-1000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-1000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-2000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-2000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-3000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-3000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-4000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-4000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-5000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-5000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-6000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-6000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-6000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 40s, sys: 6min 43s, total: 25min 23s\n",
      "Wall time: 25min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6076, training_loss=4.755729022349395, metrics={'train_runtime': 1524.2226, 'train_samples_per_second': 255.103, 'train_steps_per_second': 3.986, 'total_flos': 2276569665816576.0, 'train_loss': 4.755729022349395, 'epoch': 1.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cc07be4-60ca-4c32-84a9-9044d97dd183",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ds9_scratch/epoch_1\n",
      "Configuration saved in ds9_scratch/epoch_1/config.json\n",
      "Model weights saved in ds9_scratch/epoch_1/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(os.path.join(PATH_MODEL, \"epoch_1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecffdf2-0a1d-4b6b-a762-6f6c21ed140d",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### Test model after one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b082b9a-132c-4e2d-bdf9-f836bb138db9",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ds9_scratch/epoch_1/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ds9_scratch/epoch_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading configuration file ds9_scratch/epoch_1/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ds9_scratch/epoch_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading weights file ds9_scratch/epoch_1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ds9_scratch/epoch_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file ds9_scratch/epoch_1/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ds9_scratch/epoch_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "Didn't find file ds9_scratch/epoch_1/tokenizer.json. We won't load it.\n",
      "Didn't find file ds9_scratch/epoch_1/added_tokens.json. We won't load it.\n",
      "Didn't find file ds9_scratch/epoch_1/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ds9_scratch/epoch_1/tokenizer_config.json. We won't load it.\n",
      "loading file ds9_scratch/epoch_1/vocab.json\n",
      "loading file ds9_scratch/epoch_1/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file ds9_scratch/epoch_1/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ds9_scratch/epoch_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading configuration file ds9_scratch/epoch_1/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ds9_scratch/epoch_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=os.path.join(PATH_MODEL, \"epoch_1\"),\n",
    "    tokenizer=os.path.join(PATH_MODEL, \"epoch_1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bfd3bcd-99ec-4f1a-87cb-e9b8e2791764",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.022676831111311913,\n",
       "  'token': 355,\n",
       "  'token_str': ' it',\n",
       "  'sequence': 'He is drinking a cup of it.'},\n",
       " {'score': 0.017718225717544556,\n",
       "  'token': 578,\n",
       "  'token_str': ' them',\n",
       "  'sequence': 'He is drinking a cup of them.'},\n",
       " {'score': 0.013246491551399231,\n",
       "  'token': 434,\n",
       "  'token_str': ' him',\n",
       "  'sequence': 'He is drinking a cup of him.'},\n",
       " {'score': 0.008509969338774681,\n",
       "  'token': 643,\n",
       "  'token_str': ' us',\n",
       "  'sequence': 'He is drinking a cup of us.'},\n",
       " {'score': 0.0075829545967280865,\n",
       "  'token': 468,\n",
       "  'token_str': ' this',\n",
       "  'sequence': 'He is drinking a cup of this.'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"He is drinking a cup of <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9e41aa9-afce-45fc-b584-3b97e2234472",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.18649062514305115,\n",
       "  'token': 16,\n",
       "  'token_str': ',',\n",
       "  'sequence': 'Some beeps and the image of Gul, appears on the monitors'},\n",
       " {'score': 0.07210240513086319,\n",
       "  'token': 321,\n",
       "  'token_str': ' and',\n",
       "  'sequence': 'Some beeps and the image of Gul and appears on the monitors'},\n",
       " {'score': 0.03821699693799019,\n",
       "  'token': 349,\n",
       "  'token_str': ' is',\n",
       "  'sequence': 'Some beeps and the image of Gul is appears on the monitors'},\n",
       " {'score': 0.018567854538559914,\n",
       "  'token': 318,\n",
       "  'token_str': \"'s\",\n",
       "  'sequence': \"Some beeps and the image of Gul's appears on the monitors\"},\n",
       " {'score': 0.010578799061477184,\n",
       "  'token': 284,\n",
       "  'token_str': ' to',\n",
       "  'sequence': 'Some beeps and the image of Gul to appears on the monitors'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Some beeps and the image of Gul <mask> appears on the monitors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f512ecf7-be3c-4589-be60-9b075e922a7d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.030569445341825485,\n",
       "  'token': 557,\n",
       "  'token_str': ' get',\n",
       "  'sequence': 'Two to get up.'},\n",
       " {'score': 0.026128537952899933,\n",
       "  'token': 329,\n",
       "  'token_str': ' be',\n",
       "  'sequence': 'Two to be up.'},\n",
       " {'score': 0.02502203918993473,\n",
       "  'token': 448,\n",
       "  'token_str': ' go',\n",
       "  'sequence': 'Two to go up.'},\n",
       " {'score': 0.02097230590879917,\n",
       "  'token': 376,\n",
       "  'token_str': ' do',\n",
       "  'sequence': 'Two to do up.'},\n",
       " {'score': 0.020337576046586037,\n",
       "  'token': 591,\n",
       "  'token_str': ' see',\n",
       "  'sequence': 'Two to see up.'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Two to <mask> up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6c308cd-4086-477d-b30a-1f0a0dac63e0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.061355382204055786,\n",
       "  'token': 473,\n",
       "  'token_str': ' Sisko',\n",
       "  'sequence': 'Commander Sisko is sitting behind his desk.'},\n",
       " {'score': 0.02937745302915573,\n",
       "  'token': 588,\n",
       "  'token_str': ' Bashir',\n",
       "  'sequence': 'Commander Bashir is sitting behind his desk.'},\n",
       " {'score': 0.022974304854869843,\n",
       "  'token': 355,\n",
       "  'token_str': ' it',\n",
       "  'sequence': 'Commander it is sitting behind his desk.'},\n",
       " {'score': 0.022798290476202965,\n",
       "  'token': 564,\n",
       "  'token_str': ' Odo',\n",
       "  'sequence': 'Commander Odo is sitting behind his desk.'},\n",
       " {'score': 0.022759979590773582,\n",
       "  'token': 317,\n",
       "  'token_str': ' he',\n",
       "  'sequence': 'Commander he is sitting behind his desk.'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Commander <mask> is sitting behind his desk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "132e0f7b-fedd-4334-86d3-42d370852016",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.03109504096210003,\n",
       "  'token': 355,\n",
       "  'token_str': ' it',\n",
       "  'sequence': 'The Ferengis follow the Rules of it.'},\n",
       " {'score': 0.029616249725222588,\n",
       "  'token': 578,\n",
       "  'token_str': ' them',\n",
       "  'sequence': 'The Ferengis follow the Rules of them.'},\n",
       " {'score': 0.022989286109805107,\n",
       "  'token': 434,\n",
       "  'token_str': ' him',\n",
       "  'sequence': 'The Ferengis follow the Rules of him.'},\n",
       " {'score': 0.014342641457915306,\n",
       "  'token': 418,\n",
       "  'token_str': ' me',\n",
       "  'sequence': 'The Ferengis follow the Rules of me.'},\n",
       " {'score': 0.012819110415875912,\n",
       "  'token': 314,\n",
       "  'token_str': ' you',\n",
       "  'sequence': 'The Ferengis follow the Rules of you.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"The Ferengis follow the Rules of <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07ebf8b5-91a6-476a-9ecb-c185e1920267",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.020489269867539406,\n",
       "  'token': 792,\n",
       "  'token_str': ' door',\n",
       "  'sequence': 'In the Bajoran religion, the door is worshipped as the Celestial Temple of the Prophets'},\n",
       " {'score': 0.016336755827069283,\n",
       "  'token': 982,\n",
       "  'token_str': ' room',\n",
       "  'sequence': 'In the Bajoran religion, the room is worshipped as the Celestial Temple of the Prophets'},\n",
       " {'score': 0.011151541955769062,\n",
       "  'token': 830,\n",
       "  'token_str': ' station',\n",
       "  'sequence': 'In the Bajoran religion, the station is worshipped as the Celestial Temple of the Prophets'},\n",
       " {'score': 0.011117598041892052,\n",
       "  'token': 752,\n",
       "  'token_str': ' ship',\n",
       "  'sequence': 'In the Bajoran religion, the ship is worshipped as the Celestial Temple of the Prophets'},\n",
       " {'score': 0.0093994140625,\n",
       "  'token': 1173,\n",
       "  'token_str': ' bar',\n",
       "  'sequence': 'In the Bajoran religion, the bar is worshipped as the Celestial Temple of the Prophets'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"In the Bajoran religion, the <mask> is worshipped as the Celestial Temple of the Prophets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0308f-7a64-4905-93da-461df75e2c35",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Train for 4 more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb640cf2-214a-4721-8a7a-911502635ffb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(PATH_MODEL, \"transformer\"),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=1000,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d848464-ac5f-4d45-9ce1-9d43e36b26e3",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ds9_scratch/transformer/checkpoint-6000.\n",
      "***** Running training *****\n",
      "  Num examples = 388834\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30380\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 6000\n",
      "  Will skip the first 0 epochs then the first 6000 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017667770385742188,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 6000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a83d2a3ba0444af9bc900da73c20061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30380' max='30380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30380/30380 1:41:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.059700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.952600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.913700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.792900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.793200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.687200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.686300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.639100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.568200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.519300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.395300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.412700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.406500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.397800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.355100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.324200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.328400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.314800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.186300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.158900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.180200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.182000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-7000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-7000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-8000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-8000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-9000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-9000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-10000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-10000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-11000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-11000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-12000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-12000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-13000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-13000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-14000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-14000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-15000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-15000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-15000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-16000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-16000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-16000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-17000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-17000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-17000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-18000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-18000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-18000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-19000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-19000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-19000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-20000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-20000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-20000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-21000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-21000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-21000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-22000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-22000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-22000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-23000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-23000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-23000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-24000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-24000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-24000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-25000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-25000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-25000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-26000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-26000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-26000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-27000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-27000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-27000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-28000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-28000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-28000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-29000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-29000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-29000/pytorch_model.bin\n",
      "Saving model checkpoint to ds9_scratch/transformer/checkpoint-30000\n",
      "Configuration saved in ds9_scratch/transformer/checkpoint-30000/config.json\n",
      "Model weights saved in ds9_scratch/transformer/checkpoint-30000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 14min 28s, sys: 27min 27s, total: 1h 41min 56s\n",
      "Wall time: 1h 41min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30380, training_loss=2.769782796009659, metrics={'train_runtime': 6107.9702, 'train_samples_per_second': 318.301, 'train_steps_per_second': 4.974, 'total_flos': 1.1357226424147968e+16, 'train_loss': 2.769782796009659, 'epoch': 5.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "# trainer.train(os.path.join(PATH_MODEL, \"transformer\", \"checkpoint-6000\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdf7a5f7-c830-4bab-947b-d3d74b566af8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ds9_scratch/epoch_5\n",
      "Configuration saved in ds9_scratch/epoch_5/config.json\n",
      "Model weights saved in ds9_scratch/epoch_5/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(os.path.join(PATH_MODEL, \"epoch_5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615cf34-7792-4931-814e-70ef7944da58",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### Test model again after 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97ac4611-bd46-401f-891e-325efd0ecd1a",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ds9_scratch/epoch_5/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ds9_scratch/epoch_5\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading configuration file ds9_scratch/epoch_5/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ds9_scratch/epoch_5\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading weights file ds9_scratch/epoch_5/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ds9_scratch/epoch_5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file ds9_scratch/epoch_5/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ds9_scratch/epoch_5\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "Didn't find file ds9_scratch/epoch_5/tokenizer.json. We won't load it.\n",
      "Didn't find file ds9_scratch/epoch_5/added_tokens.json. We won't load it.\n",
      "Didn't find file ds9_scratch/epoch_5/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ds9_scratch/epoch_5/tokenizer_config.json. We won't load it.\n",
      "loading file ds9_scratch/epoch_5/vocab.json\n",
      "loading file ds9_scratch/epoch_5/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file ds9_scratch/epoch_5/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ds9_scratch/epoch_5\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading configuration file ds9_scratch/epoch_5/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ds9_scratch/epoch_5\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=os.path.join(PATH_MODEL, \"epoch_5\"),\n",
    "    tokenizer=os.path.join(PATH_MODEL, \"epoch_5\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef0a5db3-ad77-42c2-87cb-2f9253dad8b8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.019097313284873962,\n",
       "  'token': 578,\n",
       "  'token_str': ' them',\n",
       "  'sequence': 'He is drinking a cup of them.'},\n",
       " {'score': 0.017102451995015144,\n",
       "  'token': 434,\n",
       "  'token_str': ' him',\n",
       "  'sequence': 'He is drinking a cup of him.'},\n",
       " {'score': 0.013738157227635384,\n",
       "  'token': 355,\n",
       "  'token_str': ' it',\n",
       "  'sequence': 'He is drinking a cup of it.'},\n",
       " {'score': 0.010209030471742153,\n",
       "  'token': 1777,\n",
       "  'token_str': ' pain',\n",
       "  'sequence': 'He is drinking a cup of pain.'},\n",
       " {'score': 0.007656184956431389,\n",
       "  'token': 436,\n",
       "  'token_str': ' her',\n",
       "  'sequence': 'He is drinking a cup of her.'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"He is drinking a cup of <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32667974-0e26-4558-b2bd-ee81d8dadf38",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.09750576317310333,\n",
       "  'token': 473,\n",
       "  'token_str': ' Sisko',\n",
       "  'sequence': 'Some beeps and the image of Gul Sisko appears on the monitors'},\n",
       " {'score': 0.08002620190382004,\n",
       "  'token': 564,\n",
       "  'token_str': ' Odo',\n",
       "  'sequence': 'Some beeps and the image of Gul Odo appears on the monitors'},\n",
       " {'score': 0.07750523090362549,\n",
       "  'token': 588,\n",
       "  'token_str': ' Bashir',\n",
       "  'sequence': 'Some beeps and the image of Gul Bashir appears on the monitors'},\n",
       " {'score': 0.06540872901678085,\n",
       "  'token': 587,\n",
       "  'token_str': ' Quark',\n",
       "  'sequence': 'Some beeps and the image of Gul Quark appears on the monitors'},\n",
       " {'score': 0.06175588071346283,\n",
       "  'token': 584,\n",
       "  'token_str': ' Kira',\n",
       "  'sequence': 'Some beeps and the image of Gul Kira appears on the monitors'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Some beeps and the image of Gul <mask> appears on the monitors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d10d33bb-edbe-47b8-b948-0170db4fffdf",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.057491105049848557,\n",
       "  'token': 557,\n",
       "  'token_str': ' get',\n",
       "  'sequence': 'Two to get up.'},\n",
       " {'score': 0.03225880116224289,\n",
       "  'token': 1420,\n",
       "  'token_str': ' pick',\n",
       "  'sequence': 'Two to pick up.'},\n",
       " {'score': 0.027884643524885178,\n",
       "  'token': 1009,\n",
       "  'token_str': ' come',\n",
       "  'sequence': 'Two to come up.'},\n",
       " {'score': 0.025538945570588112,\n",
       "  'token': 450,\n",
       "  'token_str': ' look',\n",
       "  'sequence': 'Two to look up.'},\n",
       " {'score': 0.02337310090661049,\n",
       "  'token': 1116,\n",
       "  'token_str': ' keep',\n",
       "  'sequence': 'Two to keep up.'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Two to <mask> up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "391c3a67-3b99-410b-9a2a-455cacf295a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07920032739639282,\n",
       "  'token': 473,\n",
       "  'token_str': ' Sisko',\n",
       "  'sequence': 'Commander Sisko is sitting behind his desk.'},\n",
       " {'score': 0.07390900701284409,\n",
       "  'token': 588,\n",
       "  'token_str': ' Bashir',\n",
       "  'sequence': 'Commander Bashir is sitting behind his desk.'},\n",
       " {'score': 0.06426364928483963,\n",
       "  'token': 587,\n",
       "  'token_str': ' Quark',\n",
       "  'sequence': 'Commander Quark is sitting behind his desk.'},\n",
       " {'score': 0.042869631201028824,\n",
       "  'token': 564,\n",
       "  'token_str': ' Odo',\n",
       "  'sequence': 'Commander Odo is sitting behind his desk.'},\n",
       " {'score': 0.03325772285461426,\n",
       "  'token': 584,\n",
       "  'token_str': ' Kira',\n",
       "  'sequence': 'Commander Kira is sitting behind his desk.'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Commander <mask> is sitting behind his desk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac570eda-ad5b-4e09-b588-ce3dcbfad2cb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.15817765891551971,\n",
       "  'token': 578,\n",
       "  'token_str': ' them',\n",
       "  'sequence': 'The Ferengis follow the Rules of them.'},\n",
       " {'score': 0.05988244712352753,\n",
       "  'token': 314,\n",
       "  'token_str': ' you',\n",
       "  'sequence': 'The Ferengis follow the Rules of you.'},\n",
       " {'score': 0.05488232523202896,\n",
       "  'token': 355,\n",
       "  'token_str': ' it',\n",
       "  'sequence': 'The Ferengis follow the Rules of it.'},\n",
       " {'score': 0.03678121045231819,\n",
       "  'token': 643,\n",
       "  'token_str': ' us',\n",
       "  'sequence': 'The Ferengis follow the Rules of us.'},\n",
       " {'score': 0.03402514010667801,\n",
       "  'token': 434,\n",
       "  'token_str': ' him',\n",
       "  'sequence': 'The Ferengis follow the Rules of him.'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"The Ferengis follow the Rules of <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7451252-66c4-47b2-afa0-777c46ab310d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05264582857489586,\n",
       "  'token': 752,\n",
       "  'token_str': ' ship',\n",
       "  'sequence': 'In the Bajoran religion, the ship is worshipped as the Celestial Temple of the Prophets'},\n",
       " {'score': 0.04383635148406029,\n",
       "  'token': 982,\n",
       "  'token_str': ' room',\n",
       "  'sequence': 'In the Bajoran religion, the room is worshipped as the Celestial Temple of the Prophets'},\n",
       " {'score': 0.01855940744280815,\n",
       "  'token': 1173,\n",
       "  'token_str': ' bar',\n",
       "  'sequence': 'In the Bajoran religion, the bar is worshipped as the Celestial Temple of the Prophets'},\n",
       " {'score': 0.01752207800745964,\n",
       "  'token': 1505,\n",
       "  'token_str': ' Defiant',\n",
       "  'sequence': 'In the Bajoran religion, the Defiant is worshipped as the Celestial Temple of the Prophets'},\n",
       " {'score': 0.01479556504637003,\n",
       "  'token': 792,\n",
       "  'token_str': ' door',\n",
       "  'sequence': 'In the Bajoran religion, the door is worshipped as the Celestial Temple of the Prophets'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"In the Bajoran religion, the <mask> is worshipped as the Celestial Temple of the Prophets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5333c87-c889-4ce2-971c-0c9407827ca6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f316ec60-0424-4274-a694-ecd69ce81df2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(PATH_MODEL, \"transformer\"),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=1000,\n",
    "    prediction_loss_only=True,\n",
    "    save_total_limit=5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2462548f-0d51-49b5-83ab-decf7315228a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ds9_scratch/transformer/checkpoint-81000.\n",
      "***** Running training *****\n",
      "  Num examples = 522889\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 81710\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 9\n",
      "  Continuing training from global step 81000\n",
      "  Will skip the first 9 epochs then the first 7461 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01932978630065918,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 7461,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df451ec596444414b2cd69dbec8030ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7461 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81710' max='81710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81710/81710 02:56, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>3.237600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 32s, sys: 47.7 s, total: 3min 20s\n",
      "Wall time: 3min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=81710, training_loss=0.028166226928695998, metrics={'train_runtime': 199.225, 'train_samples_per_second': 26246.147, 'train_steps_per_second': 410.139, 'total_flos': 3.059910036677645e+16, 'train_loss': 0.028166226928695998, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train(os.path.join(PATH_MODEL, \"transformer\", \"checkpoint-81000\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79f5e023-1cdc-4424-a802-f2a8b1ba12d3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ds9_scratch/epoch_10\n",
      "Configuration saved in ds9_scratch/epoch_10/config.json\n",
      "Model weights saved in ds9_scratch/epoch_10/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(os.path.join(PATH_MODEL, \"epoch_10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33417b56-00a0-4d08-8606-2b957d753017",
   "metadata": {},
   "source": [
    "### Test model again after 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e01507-7cc6-4748-a15f-e982cbd2fa51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "PATH_MODEL = 'ds9_scratch'\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=os.path.join(PATH_MODEL, \"epoch_10\"),\n",
    "    tokenizer=os.path.join(PATH_MODEL, \"epoch_10\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e4ee6f4-e616-4066-b91f-83e3164cb3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.13619180023670197,\n",
       "  'token': 4305,\n",
       "  'token_str': ' coffee',\n",
       "  'sequence': 'He is drinking a cup of coffee.'},\n",
       " {'score': 0.03757583349943161,\n",
       "  'token': 2394,\n",
       "  'token_str': ' food',\n",
       "  'sequence': 'He is drinking a cup of food.'},\n",
       " {'score': 0.022191287949681282,\n",
       "  'token': 2319,\n",
       "  'token_str': ' latinum',\n",
       "  'sequence': 'He is drinking a cup of latinum.'},\n",
       " {'score': 0.018649088218808174,\n",
       "  'token': 5816,\n",
       "  'token_str': ' tea',\n",
       "  'sequence': 'He is drinking a cup of tea.'},\n",
       " {'score': 0.014687449671328068,\n",
       "  'token': 6112,\n",
       "  'token_str': ' raktajino',\n",
       "  'sequence': 'He is drinking a cup of raktajino.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"He is drinking a cup of <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e3f9e7-bb0f-4ed7-a7f6-bf51d5e1a921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6340900659561157,\n",
       "  'token': 1018,\n",
       "  'token_str': ' Dukat',\n",
       "  'sequence': 'Some beeps and the image of Gul Dukat appears on the monitors'},\n",
       " {'score': 0.005333753302693367,\n",
       "  'token': 2378,\n",
       "  'token_str': ' Winn',\n",
       "  'sequence': 'Some beeps and the image of Gul Winn appears on the monitors'},\n",
       " {'score': 0.004754691384732723,\n",
       "  'token': 10674,\n",
       "  'token_str': ' Toran',\n",
       "  'sequence': 'Some beeps and the image of Gul Toran appears on the monitors'},\n",
       " {'score': 0.0038876133039593697,\n",
       "  'token': 565,\n",
       "  'token_str': ' Kira',\n",
       "  'sequence': 'Some beeps and the image of Gul Kira appears on the monitors'},\n",
       " {'score': 0.0031886452343314886,\n",
       "  'token': 9628,\n",
       "  'token_str': ' Pran',\n",
       "  'sequence': 'Some beeps and the image of Gul Pran appears on the monitors'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Some beeps and the image of Gul <mask> appears on the monitors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b472be5b-c965-4339-a511-b1288d022877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.08224309980869293,\n",
       "  'token': 2170,\n",
       "  'token_str': ' beam',\n",
       "  'sequence': 'Two to beam up.'},\n",
       " {'score': 0.06752756983041763,\n",
       "  'token': 1011,\n",
       "  'token_str': ' come',\n",
       "  'sequence': 'Two to come up.'},\n",
       " {'score': 0.054405760020017624,\n",
       "  'token': 1296,\n",
       "  'token_str': ' give',\n",
       "  'sequence': 'Two to give up.'},\n",
       " {'score': 0.05349339172244072,\n",
       "  'token': 1351,\n",
       "  'token_str': ' set',\n",
       "  'sequence': 'Two to set up.'},\n",
       " {'score': 0.04312761873006821,\n",
       "  'token': 1447,\n",
       "  'token_str': ' pick',\n",
       "  'sequence': 'Two to pick up.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Two to <mask> up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f413e221-d3f1-4b5d-9f85-61ae564d51c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.11994579434394836,\n",
       "  'token': 465,\n",
       "  'token_str': ' Sisko',\n",
       "  'sequence': 'Commander Sisko is sitting behind his desk.'},\n",
       " {'score': 0.08897235989570618,\n",
       "  'token': 595,\n",
       "  'token_str': ' Bashir',\n",
       "  'sequence': 'Commander Bashir is sitting behind his desk.'},\n",
       " {'score': 0.06675644963979721,\n",
       "  'token': 572,\n",
       "  'token_str': ' Quark',\n",
       "  'sequence': 'Commander Quark is sitting behind his desk.'},\n",
       " {'score': 0.06578808277845383,\n",
       "  'token': 556,\n",
       "  'token_str': ' Odo',\n",
       "  'sequence': 'Commander Odo is sitting behind his desk.'},\n",
       " {'score': 0.05795513466000557,\n",
       "  'token': 471,\n",
       "  'token_str': ' SISKO',\n",
       "  'sequence': 'Commander SISKO is sitting behind his desk.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Commander <mask> is sitting behind his desk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9f3d0a2-546d-4057-abcb-6ee03c6d55c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5495705604553223,\n",
       "  'token': 3570,\n",
       "  'token_str': ' Acquisition',\n",
       "  'sequence': 'The Ferengis follow the Rules of Acquisition.'},\n",
       " {'score': 0.022009525448083878,\n",
       "  'token': 5141,\n",
       "  'token_str': ' Mogh',\n",
       "  'sequence': 'The Ferengis follow the Rules of Mogh.'},\n",
       " {'score': 0.01585402898490429,\n",
       "  'token': 2319,\n",
       "  'token_str': ' latinum',\n",
       "  'sequence': 'The Ferengis follow the Rules of latinum.'},\n",
       " {'score': 0.01244649849832058,\n",
       "  'token': 583,\n",
       "  'token_str': ' them',\n",
       "  'sequence': 'The Ferengis follow the Rules of them.'},\n",
       " {'score': 0.00935013685375452,\n",
       "  'token': 1340,\n",
       "  'token_str': ' course',\n",
       "  'sequence': 'The Ferengis follow the Rules of course.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"The Ferengis follow the Rules of <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40da7d44-dd41-4e1d-9080-88d58fdde892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0455084890127182,\n",
       "  'token': 784,\n",
       "  'token_str': ' man',\n",
       "  'sequence': 'In the Bajoran religion, the man is worshipped \\n             as the Celestial Temple of the Prophets'},\n",
       " {'score': 0.0448014922440052,\n",
       "  'token': 749,\n",
       "  'token_str': ' ship',\n",
       "  'sequence': 'In the Bajoran religion, the ship is worshipped \\n             as the Celestial Temple of the Prophets'},\n",
       " {'score': 0.03111078403890133,\n",
       "  'token': 970,\n",
       "  'token_str': ' room',\n",
       "  'sequence': 'In the Bajoran religion, the room is worshipped \\n             as the Celestial Temple of the Prophets'},\n",
       " {'score': 0.023898139595985413,\n",
       "  'token': 1832,\n",
       "  'token_str': ' crowd',\n",
       "  'sequence': 'In the Bajoran religion, the crowd is worshipped \\n             as the Celestial Temple of the Prophets'},\n",
       " {'score': 0.0226119477301836,\n",
       "  'token': 2770,\n",
       "  'token_str': ' Kai',\n",
       "  'sequence': 'In the Bajoran religion, the Kai is worshipped \\n             as the Celestial Temple of the Prophets'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"\"\"In the Bajoran religion, the <mask> is worshipped \n",
    "             as the Celestial Temple of the Prophets\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10ebce-5088-4a61-a327-8529030345cb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6afeba-6e60-48f0-900d-3441aec9477a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "run in terminal\n",
    "\n",
    "tensorboard dev upload --logdir ds9_scratch/transformer/runs\n",
    "https://tensorboard.dev/experiment/Vo0w6k22TRacVuzZkogjrg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698523f7-4ef5-4ad3-a29b-701b98e5f87a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
