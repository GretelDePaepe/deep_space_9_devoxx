{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "409cdb26",
   "metadata": {},
   "source": [
    "# Finetuning a masked language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbd56f",
   "metadata": {},
   "source": [
    "source: https://huggingface.co/course/chapter7/3?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a72a0b-08ff-41b8-9f1b-c72a475eb092",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f23e819",
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.4.0)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.7/site-packages (0.2.2)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /opt/conda/lib/python3.7/site-packages (4.21.1)\n",
      "Requirement already satisfied: dill<0.3.6 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (2022.8.17)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (6.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.1.97)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in /opt/conda/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.19.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.7/site-packages (0.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from accelerate) (1.21.6)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (1.12.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from accelerate) (5.9.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (4.3.0)\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]\n",
    "!pip install accelerate\n",
    "# To run the training on TPU, you will need to uncomment the followin line:\n",
    "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8770aa89-9646-4342-ab92-0b9636d456dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f3fae15",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# !git config --global user.email \"gretel_depaepe@me.com\"\n",
    "# !git config --global user.name \"Gretel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "154ca885",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888b52f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Picking a pretrained model for masked language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6221f9e0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='purple' size=4>Using a pre-trained model as is, is not difficult.  Well, except perhaps, deciding which one to choose as there are many to choose from.  We picked DistilBERT, which is a masked language model. In masked language models, random words in the input data are masked and the model needs to learn how to predict the most likely word in that spot.\n",
    "DistilBERT is trained on a dataset consisting of 11k unpublished books and English Wikipedia, just like BERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dacfc3c9",
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
      "\n",
      "All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eee947",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='purple' size=4>But, with around 67 million parameters, DistilBERT is approximately two times smaller than the BERT base model, which roughly translates into a two-fold speedup in training. That being said, be prepared to be very patient and don’t even think about retraining or training without access to at least one GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9790a1cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> DistilBERT number of parameters: 67M'\n",
      "'>>> BERT number of parameters: 110M'\n"
     ]
    }
   ],
   "source": [
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "print(f\"'>>> BERT number of parameters: 110M'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284176b2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='purple' size=4>So how good is the pre-trained model when it comes to DS9 specific data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8cb7f045",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text = \"Commander [MASK] is talking over the intercom.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e50859",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For pretrained models, the predictions depend on the corpus the model was trained on, since it learns to pick up the statistical patterns present in the data. Like BERT, DistilBERT was pretrained on the English Wikipedia and BookCorpus datasets, so we expect the predictions for [MASK] to reflect these domains. To predict the mask we need DistilBERT’s tokenizer to produce the inputs for the model, so let’s download that from the Hub as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f9b98ce",
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /home/jupyter/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /home/jupyter/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354f8ec",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With a tokenizer and a model, we can now pass our text example to the model, extract the logits, and print out the top 5 candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b281309",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Commander mccoy is talking over the intercom.'\n",
      "'>>> Commander jones is talking over the intercom.'\n",
      "'>>> Commander jameson is talking over the intercom.'\n",
      "'>>> Commander armstrong is talking over the intercom.'\n",
      "'>>> Commander vance is talking over the intercom.'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7482a74b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='purple' size=4>We can see from the outputs that the model’s predictions are based in what it learned from English Wikipedia. Let’s see how we can change this domain to something a bit more niche — Deep Space Nine Scripts!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20283fda",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05d39763",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Gretel--deep_space_9_dataset-e59f46fc56ac0435\n",
      "Reusing dataset parquet (/home/jupyter/.cache/huggingface/datasets/Gretel___parquet/Gretel--deep_space_9_dataset-e59f46fc56ac0435/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018494844436645508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f6d282d43545ef99b1e01fbc512693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'text'],\n",
       "        num_rows: 38933\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Gretel/deep_space_9_dataset\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0908af",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let’s take a look at a few samples to get an idea of what kind of text we’re dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "275d212b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/jupyter/.cache/huggingface/datasets/Gretel___parquet/Gretel--deep_space_9_dataset-e59f46fc56ac0435/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-7402f667f0a44306.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> Text:  and she looks up at'\n",
      "'>>> Title: The Circle'\n",
      "\n",
      "'>>> Text:  somehow     it has literally brought him back to     life'\n",
      "'>>> Title: Battle Lines'\n",
      "\n",
      "'>>> Text:  as Quark watches'\n",
      "'>>> Title: Emissary'\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Text: {row['text']}'\")\n",
    "    print(f\"'>>> Title: {row['title']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee56da",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff8c498",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For both auto-regressive and masked language modeling, a common preprocessing step is to concatenate all the examples and then split the whole corpus into chunks of equal size. This is quite different from our usual approach, where we simply tokenize individual examples. Why concatenate everything together? The reason is that individual examples might get truncated if they’re too long, and that would result in losing information that might be useful for the language modeling task!\n",
    "\n",
    "So to get started, we’ll first tokenize our corpus as usual, but without setting the truncation=True option in our tokenizer. We’ll also grab the word IDs if they are available, as we will need them later on to do whole word masking. We’ll wrap this in a simple function, and while we’re at it we’ll remove the text and title columns since we don’t need them any longer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "161bc976",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/Gretel___parquet/Gretel--deep_space_9_dataset-e59f46fc56ac0435/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-bcb27e138117d060.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 38933\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"title\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabc00b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we’ve tokenized our movie reviews, the next step is to group them all together and split the result into chunks. But how big should these chunks be? This will ultimately be determined by the amount of GPU memory that you have available, but a good starting point is to see what the model’s maximum context size is. This can be inferred by inspecting the model_max_length attribute of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ce0cca1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46f40a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We’ll pick something a bit smaller that can fit in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58bfe5bb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18ef93",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now comes the fun part. To show how the concatenation works, let’s take a few reviews from our tokenized training set and print out the number of tokens per review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "957d4922",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Text 0 length: 981'\n",
      "'>>> Text 1 length: 30'\n",
      "'>>> Text 2 length: 26'\n"
     ]
    }
   ],
   "source": [
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Text {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871ea25",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can then concatenate all these examples with a simple dictionary comprehension, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "851f3d8f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated text length: 1037'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated text length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8b80a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Great, the total length checks out — so now let’s split the concatenated reviews into chunks of the size given by block_size. To do so, we iterate over the features in concatenated_examples and use a list comprehension to create slices of each feature. The result is a dictionary of chunks for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a70dd8bb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 13'\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228e646",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see in this example, the last chunk will generally be smaller than the maximum chunk size. There are two main strategies for dealing with this:\n",
    "\n",
    "Drop the last chunk if it’s smaller than chunk_size.\n",
    "Pad the last chunk until its length equals chunk_size.\n",
    "We’ll take the first approach here, so let’s wrap all of the above logic in a single function that we can apply to our tokenized datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d26965f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb83c41",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that in the last step of group_texts() we create a new labels column which is a copy of the input_ids one. As we’ll see shortly, that’s because in masked language modeling the objective is to predict randomly masked tokens in the input batch, and by creating a labels column we provide the ground truth for our language model to learn from.\n",
    "\n",
    "Let’s now apply group_texts() to our tokenized datasets using our trusty Dataset.map() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74f46cff",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/Gretel___parquet/Gretel--deep_space_9_dataset-e59f46fc56ac0435/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-e2cd530ba659950d.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 20704\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e2f74f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let’s also check out what the labels look like for masked language modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b420b76b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ 25 / 92 - c star trek : deep space nine \" emissary \" cast benjamin sisko picard jake sisko con officer miles o\\'brien tactical officer kira nerys ops officer odo vulcan captain nog monk # 1 quark monk # 2 kai opaka jennifer julian bashir ferengi pit boss jadzia dax a lieutenant keiko female trans. chief gul duxat chancellor ( on monitor ) cardassian off. # 1 computer voice cardassian off. # 2 female computer voice batter alien gul jasad cardassian officer bajoran bureaucrat ( on monitor ) doran'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe354913",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ 25 / 92 - c star trek : deep space nine \" emissary \" cast benjamin sisko picard jake sisko con officer miles o\\'brien tactical officer kira nerys ops officer odo vulcan captain nog monk # 1 quark monk # 2 kai opaka jennifer julian bashir ferengi pit boss jadzia dax a lieutenant keiko female trans. chief gul duxat chancellor ( on monitor ) cardassian off. # 1 computer voice cardassian off. # 2 female computer voice batter alien gul jasad cardassian officer bajoran bureaucrat ( on monitor ) doran'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9b0ad4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As expected from our group_texts() function above, this looks identical to the decoded input_ids — but then how can our model possibly learn anything? We’re missing a key step: inserting [MASK] tokens at random positions in the inputs! Let’s see how we can do this on the fly during fine-tuning using a special data collator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d5efc9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fine-tuning a masked language model is almost identical to fine-tuning a sequence classification model. The only difference is that we need a special data collator that can randomly mask some of the tokens in each batch of texts. Fortunately, 🤗 Transformers comes prepared with a dedicated DataCollatorForLanguageModeling for just this task. We just have to pass it the tokenizer and an mlm_probability argument that specifies what fraction of the tokens to mask. We’ll pick 15%, which is the amount used for BERT and a common choice in the literature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "656503d3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba95aff",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To see how the random masking works, let’s feed a few examples to the data collator. Since it expects a list of dicts, where each dict represents a single chunk of contiguous text, we first iterate over the dataset before feeding the batch to the collator. We remove the \"word_ids\" key for this data collator as it does not expect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83f063fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] star trek : deep space [MASK] \" emissar [MASK] \" # 40511 - 721 telep [MASK] by michael piller story by rick berman michael [MASK]er the writing credits may not be final and should not be [MASK] for publicity or advertising purposes without first checking with the [MASK] legal department. copyright 1992 paramount [MASK] corporation. all rights reserved. this script is not for [MASK] or reproduction. no one is authorized to dispose of same. [MASK] lost or [MASK], please notify the script department. return to script department rev. final [MASK] paramount pictures corporation. august 10, 1992 star trek : dsaurus \" emissary [MASK] rev [MASK] final 08'\n",
      "\n",
      "'>>> / [MASK] [unused404] 92 emi c star trek [MASK] deep space [MASK] \" emissary \" cast benjamin sisko picard jake sisko con officer miles o'brien tactical [MASK] kira nerys ops officer odo [MASK] captain nog monk # 1 [MASK]ark monk # gabrielle kai op [MASK] jennifer [MASK] bashir ferengi [MASK] [MASK] jadzia dax a lieutenant keiko female trans. chief gul duxa [MASK] chancellor ( on monitor ) cardassian off. # 1 computer voice cardassian off burst # [MASK] female computer voice batter alien gul jasad cardassian officer bajoran bureau [MASK] ( on monitor ) doran'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7e108",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Nice, it worked! We can see that the [MASK] token has been randomly inserted at various locations in our text. These will be the tokens which our model will have to predict during training — and the beauty of the data collator is that it will randomize the [MASK] insertion with every batch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d7bc9f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When training models for masked language modeling, one technique that can be used is to mask whole words together, not just individual tokens. This approach is called whole word masking. If we want to use whole word masking, we will need to build a data collator ourselves. A data collator is just a function that takes a list of samples and converts them into a batch, so let’s do this now! We’ll use the word IDs computed earlier to make a map between word indices and the corresponding tokens, then randomly decide which words to mask and apply that mask on the inputs. Note that the labels are all -100 except for the ones corresponding to mask words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "891d0ba2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa1123",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, we can try it on the same samples as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bf4ca5e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] star [MASK] : deep space [MASK] \" emissary \" # 40511 - 721 teleplay by [MASK] piller story by rick berman michael piller [MASK] writing credits may not be final [MASK] should not be used for publicity or [MASK] purposes without first [MASK] with the television legal department. copyright 1992 [MASK] [MASK] corporation. all rights reserved. this script [MASK] not for publication or reproduction [MASK] no one is authorized to dispose of [MASK]. if lost [MASK] destroyed, please notify [MASK] script department. return to script department rev [MASK] final draft paramount pictures [MASK]. august 10, 1992 star trek : ds9 \" emissary \" [MASK]. final 08'\n",
      "\n",
      "'>>> / 25 / 92 - c [MASK] trek [MASK] deep [MASK] [MASK] \" [MASK] [MASK] [MASK] \" cast benjamin sisko [MASK] [MASK] jake [MASK] [MASK] [MASK] officer miles o [MASK] brien tactical officer kira nerys ops officer odo vulcan captain nog [MASK] # 1 quark monk # 2 kai opaka jennifer julian bashir ferengi pit [MASK] jadzia dax a lieutenant keiko [MASK] trans. [MASK] gul duxat chancellor ( on monitor ) cardassian off. # 1 computer voice cardassian off. # [MASK] female computer voice batter alien gul jasad cardassian officer bajoran bureaucrat [MASK] on monitor ) doran'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800f998",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we have two data collators, the rest of the fine-tuning steps are standard. Training can take a while 😭, so we’ll first downsample the size of the training set to a few thousand examples. Hopefully, we’ll still get a pretty decent language model! A quick way to downsample a dataset in 🤗 Datasets is via the Dataset.train_test_split() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe958ea9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/jupyter/.cache/huggingface/datasets/Gretel___parquet/Gretel--deep_space_9_dataset-e59f46fc56ac0435/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-c67b9104d13f3fa7.arrow and /home/jupyter/.cache/huggingface/datasets/Gretel___parquet/Gretel--deep_space_9_dataset-e59f46fc56ac0435/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-67dcdceacfe4e7a9.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 18634\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 2070\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 20704\n",
    "test_size = int(0.1 * train_size)\n",
    "train_size = train_size - test_size\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b281bd",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Fine-tuning DistilBERT with the Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1dfd83",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='purple' size=4>What happens if we try to finetune the model?  We decided to train for 20 epochs which took about an hour on a machine with 4 vCPUs, 26 GB of RAM and 1 NVIDIA Tesla T4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "936bd41f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-ds9\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=20,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    # push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a1c825",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we tweaked a few of the default options, including logging_steps to ensure we track the training loss with each epoch. We’ve also used fp16=True to enable mixed-precision training, which gives us another boost in speed. By default, the Trainer will remove any columns that are not part of the model’s forward() method. This means that if you’re using the whole word masking collator, you’ll also need to set remove_unused_columns=False to ensure we don’t lose the word_ids column during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bacf1dc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now have all the ingredients to instantiate the Trainer. Here we just use the standard data_collator, but you can try the whole word masking collator and compare the results as an exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88fc165a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=whole_word_masking_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1706b29",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Unlike other tasks like text classification or question answering where we’re given a labeled corpus to train on, with language modeling we don’t have any explicit labels. So how do we determine what makes a good language model? Like with the autocorrect feature in your phone, a good language model is one that assigns high probabilities to sentences that are grammatically correct, and low probabilities to nonsense sentences. To give you a better idea of what this looks like, you can find whole sets of “autocorrect fails” online, where the model in a person’s phone has produced some rather funny (and often inappropriate) completions!\n",
    "\n",
    "Assuming our test set consists mostly of sentences that are grammatically correct, then one way to measure the quality of our language model is to calculate the probabilities it assigns to the next word in all the sentences of the test set. High probabilities indicates that the model is not “surprised” or “perplexed” by the unseen examples, and suggests it has learned the basic patterns of grammar in the language. There are various mathematical definitions of perplexity, but the one we’ll use defines it as the exponential of the cross-entropy loss. Thus, we can calculate the perplexity of our pretrained model by using the Trainer.evaluate() function to compute the cross-entropy loss on the test set and then taking the exponential of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "233ae07c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 03:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 5.04\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a371c0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='purple' size=4>A lower perplexity score means a better language model. Let’s see if we can lower it by fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b971889",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 18634\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5840' max='5840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5840/5840 58:58, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.656200</td>\n",
       "      <td>0.526399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.524200</td>\n",
       "      <td>0.491724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.492900</td>\n",
       "      <td>0.465428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.478300</td>\n",
       "      <td>0.458215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.464700</td>\n",
       "      <td>0.451102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.453900</td>\n",
       "      <td>0.445852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.448800</td>\n",
       "      <td>0.428906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.442700</td>\n",
       "      <td>0.425711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.433600</td>\n",
       "      <td>0.422856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.430500</td>\n",
       "      <td>0.418480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>0.417563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.421600</td>\n",
       "      <td>0.416419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.416600</td>\n",
       "      <td>0.408086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.415400</td>\n",
       "      <td>0.411877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>0.412324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.411200</td>\n",
       "      <td>0.406162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.406990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.410600</td>\n",
       "      <td>0.407097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.410900</td>\n",
       "      <td>0.410482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.409832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-ds9/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-ds9/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-ds9/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-ds9/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-ds9/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-ds9/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-ds9/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-ds9/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-ds9/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-ds9/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-ds9/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-ds9/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-ds9/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-ds9/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-ds9/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-ds9/checkpoint-3000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-ds9/checkpoint-3000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-ds9/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-ds9/checkpoint-3500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-ds9/checkpoint-3500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-ds9/checkpoint-3500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-ds9/checkpoint-4000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-ds9/checkpoint-4000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-ds9/checkpoint-4000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-ds9/checkpoint-4500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-ds9/checkpoint-4500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-ds9/checkpoint-4500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-ds9/checkpoint-5000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-ds9/checkpoint-5000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-ds9/checkpoint-5000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-ds9/checkpoint-5500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-ds9/checkpoint-5500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-ds9/checkpoint-5500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5840, training_loss=0.44875546742792, metrics={'train_runtime': 3539.0753, 'train_samples_per_second': 105.304, 'train_steps_per_second': 1.65, 'total_flos': 1.235072291346432e+16, 'train_loss': 0.44875546742792, 'epoch': 20.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12da81f3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2070\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 1.50\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4442a8a8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='purple' size=4>We were indeed able to reduce the Perplexity from 5.04 to 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc3726c7-fa16-4795-ab68-69799aed2f72",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ds9_finetuned\n",
      "Configuration saved in ds9_finetuned/config.json\n",
      "Model weights saved in ds9_finetuned/pytorch_model.bin\n",
      "tokenizer config file saved in ds9_finetuned/tokenizer_config.json\n",
      "Special tokens file saved in ds9_finetuned/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ds9_finetuned/tokenizer_config.json',\n",
       " 'ds9_finetuned/special_tokens_map.json',\n",
       " 'ds9_finetuned/vocab.txt',\n",
       " 'ds9_finetuned/added_tokens.json',\n",
       " 'ds9_finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"ds9_finetuned\")\n",
    "tokenizer.save_pretrained(\"ds9_finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369bac3-2ddd-486a-ac56-19d2c25c5a6c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c20790ac-d558-4239-9917-fbbbc3d6100c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ds9_finetuned/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"ds9_finetuned\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file ds9_finetuned/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"ds9_finetuned\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ds9_finetuned/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
      "\n",
      "All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at ds9_finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n",
      "Didn't find file ds9_finetuned/added_tokens.json. We won't load it.\n",
      "loading file ds9_finetuned/vocab.txt\n",
      "loading file ds9_finetuned/tokenizer.json\n",
      "loading file None\n",
      "loading file ds9_finetuned/special_tokens_map.json\n",
      "loading file ds9_finetuned/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", model=\"ds9_finetuned\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2ef1907-9d35-45cd-864e-daa00d9a9f47",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> commander kira is talking over the intercom.\n",
      ">>> commander dax is talking over the intercom.\n",
      ">>> commander ross is talking over the intercom.\n",
      ">>> commander carlson is talking over the intercom.\n",
      ">>> commander wainwright is talking over the intercom.\n"
     ]
    }
   ],
   "source": [
    "text = \"Commander [MASK] is talking over the intercom.\"\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89522bc2-9cef-4373-9060-b8f22c2243c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> he is drinking a cup of coffee.\n",
      ">>> he is drinking a cup of tea.\n",
      ">>> he is drinking a cup of water.\n",
      ">>> he is drinking a cup of ale.\n",
      ">>> he is drinking a cup of wine.\n"
     ]
    }
   ],
   "source": [
    "text = \"He is drinking a cup of [MASK].\"\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8cd23f65-0a65-49ed-afec-19ab6a512db9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> some beeps and the image of gul amin appears on the monitors\n",
      ">>> some beeps and the image of gul du appears on the monitors\n",
      ">>> some beeps and the image of gul omar appears on the monitors\n",
      ">>> some beeps and the image of gul goran appears on the monitors\n",
      ">>> some beeps and the image of gul ali appears on the monitors\n"
     ]
    }
   ],
   "source": [
    "text = \"Some beeps and the image of Gul [MASK] appears on the monitors\"\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a34bd668-15b7-4a00-852b-41c9ac6c3ef0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> two to give up.\n",
      ">>> two to catch up.\n",
      ">>> two to stand up.\n",
      ">>> two to set up.\n",
      ">>> two to one up.\n"
     ]
    }
   ],
   "source": [
    "text = \"Two to [MASK] up.\"\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c66e6592-dc36-4bd0-9f44-4f6032c2d130",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> the ferengis follow the rules of acquisition.\n",
      ">>> the ferengis follow the rules of obedience.\n",
      ">>> the ferengis follow the rules of battle.\n",
      ">>> the ferengis follow the rules of nature.\n",
      ">>> the ferengis follow the rules of existence.\n"
     ]
    }
   ],
   "source": [
    "text = \"The Ferengis follow the Rules of [MASK].\"\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3e7952a-588e-4795-abf5-5e29d9749f8e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> in the bajoran religion, the shrine is worshipped as the celestial temple of the prophets\n",
      ">>> in the bajoran religion, the temple is worshipped as the celestial temple of the prophets\n",
      ">>> in the bajoran religion, the moon is worshipped as the celestial temple of the prophets\n",
      ">>> in the bajoran religion, the place is worshipped as the celestial temple of the prophets\n",
      ">>> in the bajoran religion, the mountain is worshipped as the celestial temple of the prophets\n"
     ]
    }
   ],
   "source": [
    "text = \"In the Bajoran religion, the [MASK] is worshipped as the Celestial Temple of the Prophets\"\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000fb94",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='purple' size=4> As you can see, the results are a bit disappointing.  Now obviously one could spend days or even weeks trying out different hyperparameters etc.  And it has to be said that many claim to get pretty decent result fine tuning a pre-trained model, especially when trained afterwards on anothe task such as a classification for example.  It all depends on the specific use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40e5ab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
